# 概览

决策树(Decision Trees)是一个贪心算法, 它在特征空间上执行递归的二元分割. 决策树由节点和有向边组成, 内部节点表示一个特征或属性, 叶子节点表示一个分类. 决策树本质就是一系列的 if-then-else 语句.

构建决策树通常包含三个步骤:

1. 特征选择
2. 决策树生成
3. 决策树剪枝

构建决策树时通常将正则化的极大似然函数作为损失函数, 其学习目标是损失函数为目标函数的最小化. 构建决策树的算法通常是递归地选择最优特征, 并根据该特征对训练数据集进行分割, 其步骤如下:

1. 构建根节点，所有训练样本都位于根节点.
2. 选择一个最优特征. 通过该特征将训练数据分割成子集, 确保各个子集有最好的分类, 但要考虑下列两种情况:
    1. 若子集已能够被"较好地"分类, 则构建叶节点, 并将该子集划分到对应的叶节点去
    2. 若某个子集不能够被"较好地"分类, 则对该子集继续划分
3. 递归直至所有训练样本都被较好地分类, 或者没有合适的特征为止. 是否"较好地"分类, 可通过后面介绍的指标来判断.

通过该步骤生成的决策树对训练样本有很好的分类能力, 但是我们需要的是对未知样本的分类能力. 因此通常需要对已生成的决策树进行剪枝, 从而使得决策树具有更好的泛化能力. 剪枝过程是去掉过于细分的叶节点, 从而提高泛化能力.

特征选择主要基于以下几个指标: 熵, 基尼系数和方差.

# 熵

回忆一下信息论中有关熵(也就是信息量)的定义. 设 X 是一个离散型随机变量, 其概率分布为

$$
P(X=x_i) = p_i, i=1, 2, 3, .., n
$$

则随机变量的熵为

$$
H(X) = - sum_(i=1)^np_ilogp_i
$$

其中, 定义 0log0 = 0.

**举个栗子**

有变量 X, 它可能的取值有 0, 1, 2 三种, 其概率分别是 0.25, 0.5 和 0.25. 那么 X 的熵为: $$H(X) = -(1/4log1/4 + 1/2log1/2 + 1/4log1/4) = $$

# 参考

- [1] sklearn: 决策树 [http://sklearn.apachecn.org/cn/0.19.0/modules/tree.html](http://sklearn.apachecn.org/cn/0.19.0/modules/tree.html)
- [2] 华校专/王正林: Python大战机器学习.第二章.决策树
